{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from classes.intrafeature import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea\n",
    "\n",
    "Submissions are limited, thus we need a way to quickly screen if a prediction is usable as anomaly detector\n",
    "\n",
    "**Given** Assumed ratio of anomalies R\n",
    "\n",
    "* For each feature in group of features:\n",
    "    * Predict feature based on remaining features\n",
    "    * Classify everything in the R quantile and 1-R quantile as anomalies\n",
    "    * Evaluate plausibility of anomaly detector (e.g., how coherent are the anomalies?)\n",
    "\n",
    "### Plausibility\n",
    "I assume plausibility to be reflected in the distribution of block-lengths of connected anomalies.\n",
    "For example: A detector that produces mainly isolated one-point anomalies is nonsense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configuration and group of features to investigate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "exclude_features = [\"id\", \"Datum_Uhrzeit_CET\", \"controlArea\", \"participationCMO\", \"participationIN\"]\n",
    "all_features = [feature for feature in train_df.columns if not feature in exclude_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"max_depth\": 5}\n",
    "regression_class = DecisionTreeRegressor\n",
    "group = [\"aFRRactivation\", \"aFRRrequest\"]#\"correction\", \"correctedDemand\", \"Demand\"]\n",
    "\n",
    "groups = [group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_control_area = test_df.controlArea == 1\n",
    "\n",
    "for group in tqdm.tqdm(combinations(all_features, 2)):\n",
    "    print(f\"Group {group}\")\n",
    "    model_dict = train_intrafeature(train_df, \n",
    "                                test_df,\n",
    "                                group=group, \n",
    "                                regression_class=regression_class, \n",
    "                                hyperparameters=hyperparameters,\n",
    "                                metric=r2_score)\n",
    "    \n",
    "    best_target_name = max(model_dict, key=lambda x: model_dict[x][2])\n",
    "    model, remaining, train_score, val_score = model_dict[best_target_name]\n",
    "    if val_score < 0.8:\n",
    "        print(f\"Val score {val_score}. Skipping\")\n",
    "    else:\n",
    "        print(f\"Target {best_target_name} with scores ({train_score} | {val_score})\")\n",
    "    target, pred = run_intrafeature_model(test_df, model, best_target_name, remaining)\n",
    "    anomalies = hypothesize_anomalies(target[selector_control_area], pred[selector_control_area], 0.15)\n",
    "    anomalies = anomalies.rename(\"anomaly\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_target_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_anomalies(df, window_size=4, threshold=2, loops=2):\n",
    "    count = 0\n",
    "    for l in range(loops):\n",
    "        for index, (_, row) in enumerate(df.iterrows()):\n",
    "            if row[\"anomaly\"] == 0:\n",
    "                print(index)\n",
    "                start_index = max(index - window_size, 0)\n",
    "                end_index = min(index + window_size + 1, len(df))\n",
    "                window = df[\"anomaly\"][start_index:end_index]\n",
    "                # Prüfe, ob mindestens eine '1' im Bereich vor und nach der '0' ist\n",
    "                if 1 in df[\"anomaly\"][start_index:index].values and 1 in df[\"anomaly\"][index + 1: end_index].values:\n",
    "                    window = df[\"anomaly\"][start_index:end_index]\n",
    "                    if window.sum() >= threshold:\n",
    "                        df.at[index, \"anomaly\"] = 1\n",
    "                        count += 1\n",
    "    print(\"Gefüllt:\", count)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_anomalies(df, window_size=5, threshold=1):\n",
    "    count = 0\n",
    "    for index, (_, row) in enumerate(df.iterrows()):\n",
    "        if row[\"anomaly\"] == 1:\n",
    "            start_index = max(index - window_size, 0)\n",
    "            end_index = min(index + window_size + 1, len(df))\n",
    "\n",
    "            if 0 in df[\"anomaly\"][start_index:index].values and 0 in df[\"anomaly\"][index + 1: end_index].values:\n",
    "                window = df[\"anomaly\"][start_index:end_index]\n",
    "                if window.sum() <= threshold:\n",
    "                    df.at[index, \"anomaly\"] = 0\n",
    "                    count += 1\n",
    "    print(\"Entfernt:\", count)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_filled = fill_anomalies(\n",
    "    pd.DataFrame(anomalies).copy(), window_size=10, threshold=4, loops=2)\n",
    "submission_df = remove_anomalies(\n",
    "    df_filled.copy(), window_size=5, threshold=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 500\n",
    "counts = count_anomagrams(submission_df[\"anomaly\"])\n",
    "counts = counts[counts < cutoff]\n",
    "hist = plt.hist(np.log(counts), bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"a\": (2, 4), \"b\": (3, 2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
