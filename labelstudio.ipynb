{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X GET https://data-science-kitchen-labelstudio.hf.space/ -H 'Authorization: Token b26bd6be6a16e42663f9de0ad0d782b673ece82b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'csv': '/data/upload/1/8f2cb197-train_0000.csv'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from label_studio_sdk import Client\n",
    "\n",
    "LABEL_STUDIO_URL = 'https://data-science-kitchen-labelstudio.hf.space'\n",
    "API_KEY = 'b26bd6be6a16e42663f9de0ad0d782b673ece82b'\n",
    "\n",
    "ls = Client(url=LABEL_STUDIO_URL, api_key=API_KEY)\n",
    "ls.check_connection()\n",
    "project = ls.get_project(1)\n",
    "task_ids = project.get_tasks_ids()\n",
    "# Examplen to get the CSV file path\n",
    "project.get_task(task_ids[0])['data']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IsolationForest(n_estimators=32, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IsolationForest</label><div class=\"sk-toggleable__content\"><pre>IsolationForest(n_estimators=32, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "IsolationForest(n_estimators=32, random_state=42)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from classes.feature_factory import FeatureFactory\n",
    "\n",
    "# Trainingsdaten & Testdaten laden\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# Beispiel für die Verwendung FeatureFactory\n",
    "factory_train = FeatureFactory(train_df, None)\n",
    "\n",
    "n_sfa_components = 4\n",
    "sfa_control_areas = [1, 2]\n",
    "sfa_degree = 2\n",
    "\n",
    "selected_sfa_features = [\"Demand\", \"correction\", \"correctionEcho\",\n",
    "                            \"FRCE\", \"LFCInput\", \"aFRRactivation\", \"aFRRrequest\"]\n",
    "trained_sfa_pipelines = []\n",
    "for sfa_control_area in sfa_control_areas:\n",
    "    pipeline = factory_train.add_SFA(\n",
    "        n_sfa_components,\n",
    "        selected_sfa_features,\n",
    "        poly_degree=sfa_degree,\n",
    "        control_area=sfa_control_area,\n",
    "        batch_size=100,\n",
    "        cascade_length=1,\n",
    "    )\n",
    "    trained_sfa_pipelines.append(pipeline)\n",
    "\n",
    "factory_train.add_corrected_demand_feature()\n",
    "factory_train.add_time_features()\n",
    "factory_train.add_rolling_features(window_size=3)\n",
    "# factory.add_rolling_features_by_control_area(window_size=3)\n",
    "factory_train.add_ratio_and_diff_features()\n",
    "factory_train.add_aFRR_activation_request_ratio()\n",
    "factory_train.add_FRCE_LFCInput_difference()\n",
    "factory_train.add_participation_state()\n",
    "factory_train.add_demand_FRCE_interaction()\n",
    "\n",
    "# Features\n",
    "# New features beginn with 'day', ...\n",
    "features = [\n",
    "    \"Demand\",\n",
    "    \"correction\",\n",
    "    \"correctedDemand\",\n",
    "    \"FRCE\",\n",
    "    \"controlBandPos\",\n",
    "    \"controlBandNeg\",\n",
    "    \"LFCInput\",\n",
    "    \"aFRRactivation\",\n",
    "    \"aFRRrequest\",\n",
    "    \"participationCMO\",\n",
    "    \"participationIN\",\n",
    "    \"correctionEcho\",\n",
    "    \"BandLimitedCorrectedDemand\",\n",
    "    \"controlArea\",\n",
    "    \"hour\",\n",
    "    \"day\",\n",
    "    \"weekday\",\n",
    "    \"month\",\n",
    "    # \"Demand_RollingMean\",\n",
    "    # \"Demand_RollingStd\",\n",
    "    \"Demand_CorrectedDemand_Ratio\",\n",
    "    \"Demand_CorrectedDemand_Diff\",\n",
    "    \"aFRR_Activation_Request_Ratio\",\n",
    "    \"FRCE_LFCInput_Diff\",\n",
    "    \"Participation_State\",\n",
    "    \"Demand_FRCE_Interaction\",\n",
    "]  # , 'corrected_demand_diff']\n",
    "\n",
    "for sfa_control_area in sfa_control_areas:\n",
    "    sfa_features = [\n",
    "        f\"sfa{c}_{sfa_control_area}\" for c in range(n_sfa_components)]\n",
    "features = features + sfa_features\n",
    "\n",
    "X_train = factory_train.train_data[features]\n",
    "\n",
    "X_train.isna().sum()\n",
    "\n",
    "# Scaler\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "\n",
    "# Isolation Forest Modell initialisieren und trainieren\n",
    "model = IsolationForest(\n",
    "    n_estimators=32, contamination=\"auto\", random_state=42)\n",
    "model.fit(X_train_normalized)\n",
    "\n",
    "#submission_df = submission_df[[\"id\", \"anomaly\"]]\n",
    "#submission_df.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_anomalies(df, window_size=4, threshold=2, loops=2):\n",
    "    count = 0\n",
    "    for l in range(loops):\n",
    "        for index, row in df.iterrows():\n",
    "            if row[\"anomaly\"] == 0:\n",
    "                start_index = max(index - window_size, 0)\n",
    "                end_index = min(index + window_size + 1, len(df))\n",
    "                window = df[\"anomaly\"][start_index:end_index]\n",
    "\n",
    "                # Prüfe, ob mindestens eine '1' im Bereich vor und nach der '0' ist\n",
    "                if 1 in df[\"anomaly\"][start_index:index].values and 1 in df[\"anomaly\"][index + 1: end_index].values:\n",
    "                    window = df[\"anomaly\"][start_index:end_index]\n",
    "                    if window.sum() >= threshold:\n",
    "                        df.at[index, \"anomaly\"] = 1\n",
    "                        count += 1\n",
    "    print(\"Gefüllt:\", count)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_anomalies(df, window_size=5, threshold=1):\n",
    "    count = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"anomaly\"] == 1:\n",
    "            start_index = max(index - window_size, 0)\n",
    "            end_index = min(index + window_size + 1, len(df))\n",
    "\n",
    "            if 0 in df[\"anomaly\"][start_index:index].values and 0 in df[\"anomaly\"][index + 1: end_index].values:\n",
    "                window = df[\"anomaly\"][start_index:end_index]\n",
    "                if window.sum() <= threshold:\n",
    "                    df.at[index, \"anomaly\"] = 0\n",
    "                    count += 1\n",
    "    print(\"Entfernt:\", count)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/Downloads/test_segments/test_0000.csv\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileid 0\n",
      "Gefüllt: 41\n",
      "Entfernt: 4\n",
      "(Timestamp('2023-05-13 00:13:08'), Timestamp('2023-05-13 00:24:56'))\n",
      "(Timestamp('2023-05-13 00:31:12'), Timestamp('2023-05-13 00:56:16'))\n",
      "(Timestamp('2023-05-13 01:59:08'), Timestamp('2023-05-13 01:59:32'))\n",
      "fileid 1\n",
      "Gefüllt: 40\n",
      "Entfernt: 2\n",
      "(Timestamp('2023-05-31 15:53:36'), Timestamp('2023-05-31 16:05:52'))\n",
      "fileid 2\n",
      "Gefüllt: 1\n",
      "Entfernt: 0\n",
      "(Timestamp('2023-05-29 19:41:32'), Timestamp('2023-05-29 19:56:40'))\n",
      "(Timestamp('2023-05-29 20:00:36'), Timestamp('2023-05-29 20:01:08'))\n",
      "fileid 3\n",
      "Gefüllt: 108\n",
      "Entfernt: 4\n",
      "(Timestamp('2023-05-08 11:04:24'), Timestamp('2023-05-08 11:08:08'))\n",
      "(Timestamp('2023-05-08 11:10:24'), Timestamp('2023-05-08 11:13:20'))\n",
      "(Timestamp('2023-05-08 11:23:24'), Timestamp('2023-05-08 11:43:00'))\n",
      "(Timestamp('2023-05-08 11:45:12'), Timestamp('2023-05-08 11:46:32'))\n",
      "(Timestamp('2023-05-08 11:47:52'), Timestamp('2023-05-08 11:48:48'))\n",
      "(Timestamp('2023-05-08 12:00:08'), Timestamp('2023-05-08 12:04:12'))\n",
      "fileid 4\n",
      "Gefüllt: 107\n",
      "Entfernt: 8\n",
      "(Timestamp('2023-06-04 13:00:32'), Timestamp('2023-06-04 13:03:52'))\n",
      "(Timestamp('2023-06-04 13:11:32'), Timestamp('2023-06-04 13:13:16'))\n",
      "(Timestamp('2023-06-04 13:16:12'), Timestamp('2023-06-04 13:18:32'))\n",
      "(Timestamp('2023-06-04 13:21:04'), Timestamp('2023-06-04 13:21:52'))\n",
      "(Timestamp('2023-06-04 13:47:40'), Timestamp('2023-06-04 13:48:24'))\n",
      "(Timestamp('2023-06-04 13:49:52'), Timestamp('2023-06-04 13:51:32'))\n",
      "(Timestamp('2023-06-04 13:52:44'), Timestamp('2023-06-04 13:53:36'))\n",
      "(Timestamp('2023-06-04 14:00:20'), Timestamp('2023-06-04 14:04:24'))\n",
      "fileid 5\n",
      "Gefüllt: 1\n",
      "Entfernt: 2\n",
      "(Timestamp('2023-06-04 16:03:04'), Timestamp('2023-06-04 16:03:24'))\n",
      "fileid 6\n",
      "Gefüllt: 1\n",
      "Entfernt: 0\n",
      "(Timestamp('2023-05-29 19:41:32'), Timestamp('2023-05-29 19:56:40'))\n",
      "(Timestamp('2023-05-29 20:00:36'), Timestamp('2023-05-29 20:01:08'))\n",
      "fileid 7\n",
      "Gefüllt: 36\n",
      "Entfernt: 0\n",
      "(Timestamp('2023-04-25 16:30:56'), Timestamp('2023-04-25 16:38:16'))\n",
      "fileid 8\n",
      "Gefüllt: 98\n",
      "Entfernt: 5\n",
      "(Timestamp('2023-05-07 11:52:36'), Timestamp('2023-05-07 11:59:12'))\n",
      "(Timestamp('2023-05-07 12:15:20'), Timestamp('2023-05-07 12:20:12'))\n",
      "(Timestamp('2023-05-07 12:22:44'), Timestamp('2023-05-07 12:26:56'))\n",
      "(Timestamp('2023-05-07 12:36:56'), Timestamp('2023-05-07 12:39:48'))\n",
      "fileid 9\n",
      "Gefüllt: 42\n",
      "Entfernt: 7\n",
      "(Timestamp('2023-05-11 23:41:28'), Timestamp('2023-05-11 23:41:32'))\n",
      "(Timestamp('2023-05-11 23:53:56'), Timestamp('2023-05-11 23:54:44'))\n",
      "(Timestamp('2023-05-12 00:00:16'), Timestamp('2023-05-12 00:01:00'))\n",
      "(Timestamp('2023-05-12 00:09:52'), Timestamp('2023-05-12 00:13:16'))\n",
      "fileid 10\n",
      "Gefüllt: 82\n",
      "Entfernt: 6\n",
      "(Timestamp('2023-05-29 02:22:16'), Timestamp('2023-05-29 02:39:16'))\n",
      "(Timestamp('2023-05-29 03:04:08'), Timestamp('2023-05-29 03:09:12'))\n",
      "fileid 11\n",
      "Gefüllt: 15\n",
      "Entfernt: 0\n",
      "(Timestamp('2023-05-27 02:12:40'), Timestamp('2023-05-27 02:18:52'))\n",
      "(Timestamp('2023-05-27 02:43:12'), Timestamp('2023-05-27 02:55:24'))\n",
      "fileid 12\n",
      "Gefüllt: 77\n",
      "Entfernt: 4\n",
      "(Timestamp('2023-05-02 21:43:08'), Timestamp('2023-05-02 21:44:04'))\n",
      "(Timestamp('2023-05-02 21:51:04'), Timestamp('2023-05-02 21:51:40'))\n",
      "(Timestamp('2023-05-02 22:02:32'), Timestamp('2023-05-02 22:12:28'))\n",
      "(Timestamp('2023-05-02 22:59:12'), Timestamp('2023-05-02 23:17:32'))\n",
      "(Timestamp('2023-05-02 23:23:44'), Timestamp('2023-05-02 23:26:00'))\n",
      "fileid 13\n",
      "Gefüllt: 91\n",
      "Entfernt: 3\n",
      "(Timestamp('2023-05-09 16:00:04'), Timestamp('2023-05-09 16:00:32'))\n",
      "(Timestamp('2023-05-09 16:24:56'), Timestamp('2023-05-09 17:05:04'))\n",
      "fileid 14\n",
      "Gefüllt: 216\n",
      "Entfernt: 4\n",
      "(Timestamp('2023-05-23 23:37:56'), Timestamp('2023-05-23 23:38:40'))\n",
      "(Timestamp('2023-05-23 23:42:52'), Timestamp('2023-05-23 23:46:00'))\n",
      "(Timestamp('2023-05-23 23:59:00'), Timestamp('2023-05-24 00:00:20'))\n",
      "(Timestamp('2023-05-24 00:03:32'), Timestamp('2023-05-24 00:07:12'))\n",
      "(Timestamp('2023-05-24 00:41:12'), Timestamp('2023-05-24 00:51:04'))\n",
      "(Timestamp('2023-05-24 00:52:40'), Timestamp('2023-05-24 00:54:44'))\n",
      "(Timestamp('2023-05-24 00:56:44'), Timestamp('2023-05-24 01:01:08'))\n",
      "(Timestamp('2023-05-24 01:09:56'), Timestamp('2023-05-24 01:12:08'))\n",
      "(Timestamp('2023-05-24 01:13:44'), Timestamp('2023-05-24 01:15:48'))\n",
      "fileid 15\n",
      "Gefüllt: 17\n",
      "Entfernt: 10\n",
      "(Timestamp('2023-07-01 11:29:12'), Timestamp('2023-07-01 11:31:16'))\n",
      "fileid 16\n",
      "Gefüllt: 122\n",
      "Entfernt: 6\n",
      "(Timestamp('2023-06-28 09:09:04'), Timestamp('2023-06-28 09:09:32'))\n",
      "(Timestamp('2023-06-28 09:15:44'), Timestamp('2023-06-28 09:22:04'))\n",
      "(Timestamp('2023-06-28 09:30:36'), Timestamp('2023-06-28 09:43:32'))\n",
      "(Timestamp('2023-06-28 09:44:44'), Timestamp('2023-06-28 09:45:56'))\n",
      "(Timestamp('2023-06-28 10:02:32'), Timestamp('2023-06-28 10:05:24'))\n",
      "fileid 17\n",
      "Gefüllt: 44\n",
      "Entfernt: 0\n",
      "(Timestamp('2023-06-03 05:39:00'), Timestamp('2023-06-03 05:40:40'))\n",
      "(Timestamp('2023-06-03 06:19:04'), Timestamp('2023-06-03 07:30:44'))\n",
      "(Timestamp('2023-06-03 07:35:08'), Timestamp('2023-06-03 07:35:12'))\n",
      "(Timestamp('2023-06-03 07:35:16'), Timestamp('2023-06-03 07:37:16'))\n",
      "fileid 18\n",
      "Gefüllt: 159\n",
      "Entfernt: 4\n",
      "(Timestamp('2023-06-30 09:02:32'), Timestamp('2023-06-30 09:09:28'))\n",
      "(Timestamp('2023-06-30 09:14:28'), Timestamp('2023-06-30 09:18:40'))\n",
      "(Timestamp('2023-06-30 09:34:04'), Timestamp('2023-06-30 09:35:24'))\n",
      "(Timestamp('2023-06-30 10:02:32'), Timestamp('2023-06-30 10:10:24'))\n",
      "(Timestamp('2023-06-30 10:17:24'), Timestamp('2023-06-30 10:18:00'))\n",
      "fileid 19\n",
      "Gefüllt: 203\n",
      "Entfernt: 10\n",
      "(Timestamp('2023-05-03 11:43:28'), Timestamp('2023-05-03 11:52:52'))\n",
      "(Timestamp('2023-05-03 12:00:36'), Timestamp('2023-05-03 12:17:32'))\n",
      "(Timestamp('2023-05-03 12:30:28'), Timestamp('2023-05-03 13:12:16'))\n",
      "(Timestamp('2023-05-03 13:15:28'), Timestamp('2023-05-03 13:20:48'))\n",
      "fileid 20\n",
      "Gefüllt: 170\n",
      "Entfernt: 15\n",
      "(Timestamp('2023-05-03 10:46:28'), Timestamp('2023-05-03 10:48:40'))\n",
      "(Timestamp('2023-05-03 10:55:36'), Timestamp('2023-05-03 10:56:28'))\n",
      "(Timestamp('2023-05-03 10:59:56'), Timestamp('2023-05-03 11:52:52'))\n",
      "(Timestamp('2023-05-03 12:00:36'), Timestamp('2023-05-03 12:17:32'))\n"
     ]
    }
   ],
   "source": [
    "for fileid in range(21):\n",
    "    test_df = pd.read_csv(f\"~/Downloads/test_segments/test_00{fileid:02}.csv\")\n",
    "    print('fileid '+str(fileid))\n",
    "    # Beispiel für die Verwendung FeatureFactory\n",
    "    factory_test = FeatureFactory(None, test_df)\n",
    "\n",
    "    n_sfa_components = 4\n",
    "    sfa_control_areas = [1, 2]\n",
    "    sfa_degree = 2\n",
    "\n",
    "    selected_sfa_features = [\"Demand\", \"correction\", \"correctionEcho\",\n",
    "                                \"FRCE\", \"LFCInput\", \"aFRRactivation\", \"aFRRrequest\"]\n",
    "\n",
    "    for sfa_control_area, trained_sfa_pipeline in zip(sfa_control_areas,trained_sfa_pipelines):\n",
    "        factory_test.add_SFA(\n",
    "            n_sfa_components,\n",
    "            selected_sfa_features,\n",
    "            poly_degree=sfa_degree,\n",
    "            control_area=sfa_control_area,\n",
    "            batch_size=100,\n",
    "            cascade_length=1,\n",
    "            trained_pipeline=trained_sfa_pipeline\n",
    "        )\n",
    "\n",
    "    factory_test.add_corrected_demand_feature()\n",
    "    factory_test.add_time_features()\n",
    "    factory_test.add_rolling_features(window_size=3)\n",
    "    factory_test.add_ratio_and_diff_features()\n",
    "    factory_test.add_aFRR_activation_request_ratio()\n",
    "    factory_test.add_FRCE_LFCInput_difference()\n",
    "    factory_test.add_participation_state()\n",
    "    factory_test.add_demand_FRCE_interaction()\n",
    "\n",
    "    X_test = factory_test.test_data[features]\n",
    "\n",
    "    X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "    # Anomalien auf Testdaten vorhersagen und anzeigen\n",
    "    test_df[\"anomaly\"] = model.predict(X_test_normalized)\n",
    "\n",
    "    # Konvertiere Anomalie-Vorhersagen: -1 (Anomalie) wird zu 1 und 1 (normal) wird zu 0\n",
    "    test_df[\"anomaly\"] = test_df[\"anomaly\"].apply(\n",
    "        lambda x: 1 if x == -1 else 0)\n",
    "\n",
    "    df_filled = fill_anomalies(\n",
    "        test_df.copy(), window_size=10, threshold=4, loops=2)\n",
    "    submission_df = remove_anomalies(\n",
    "        df_filled.copy(), window_size=5, threshold=4)\n",
    "\n",
    "    submission_df['shifted_anomaly'] = submission_df['anomaly'].shift(1)\n",
    "    submission_df['change'] = submission_df['anomaly'] != submission_df['shifted_anomaly']\n",
    "    changes_df = submission_df[submission_df['change']]\n",
    "\n",
    "    windows = []\n",
    "    for i in range(len(changes_df) - 1):\n",
    "        start_time = changes_df.iloc[i]['Datum_Uhrzeit_CET']\n",
    "        end_time = changes_df.iloc[i + 1]['Datum_Uhrzeit_CET']\n",
    "        start_anomaly = changes_df.iloc[i]['anomaly']\n",
    "        end_anomaly = changes_df.iloc[i + 1]['anomaly']\n",
    "        \n",
    "        # Assuming anomaly switches from 0 to 1 and then back from 1 to 0\n",
    "        if start_anomaly == 1 and end_anomaly == 0:\n",
    "            windows.append((start_time, end_time))\n",
    "\n",
    "    result=[]\n",
    "    for window in windows:\n",
    "        print(window)\n",
    "        result.append(\n",
    "            { \n",
    "                \"from_name\": \"label\",\n",
    "                \"to_name\": \"ts\",\n",
    "                \"type\": \"timeserieslabels\",\n",
    "                \"value\": {\n",
    "                    \"start\": str(window[0]),  \n",
    "                    \"end\": str(window[1]),    \n",
    "                    \"timeserieslabels\": [\"anomaly\"] \n",
    "                \n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    project.create_prediction(fileid+2, \n",
    "                                result=result, \n",
    "                                model_version='1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
